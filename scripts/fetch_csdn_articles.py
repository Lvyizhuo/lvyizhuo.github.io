#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CSDNåšå®¢æ–‡ç« æŠ“å–è„šæœ¬
ä½¿ç”¨ CSDN API æˆ– HTML è§£ææŠ“å–æŒ‡å®šåšå®¢çš„æ‰€æœ‰æ–‡ç« åˆ—è¡¨å¹¶ä¿å­˜ä¸ºYAMLæ ¼å¼
ä½œè€…: GitHub Actions Bot
"""

import requests
import yaml
import json
from datetime import datetime
import time
import os
from bs4 import BeautifulSoup
import re

# CSDNåšå®¢é…ç½®
CSDN_USERNAME = "Lvyizhuo"
CSDN_BLOG_URL = f"https://blog.csdn.net/{CSDN_USERNAME}"
# CSDN å†…éƒ¨ API
CSDN_ARTICLE_LIST_API = "https://blog.csdn.net/community/home-api/v1/get-business-list"
OUTPUT_FILE = "_data/csdn_posts.yml"

# è¯·æ±‚å¤´ - æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
    'Accept-Encoding': 'gzip, deflate, br',
    'Connection': 'keep-alive',
    'Cache-Control': 'max-age=0',
    'Upgrade-Insecure-Requests': '1',
    'Referer': 'https://blog.csdn.net/',
}

# API ä¸“ç”¨è¯·æ±‚å¤´
API_HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'application/json, text/plain, */*',
    'Accept-Language': 'zh-CN,zh;q=0.9',
    'Referer': f'https://blog.csdn.net/{CSDN_USERNAME}',
    'Origin': 'https://blog.csdn.net',
}


def fetch_article_list_from_html():
    """
    ä½¿ç”¨ HTML è§£ææ–¹å¼æŠ“å–åšå®¢æ–‡ç« åˆ—è¡¨ï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰
    
    Returns:
        articles: æ–‡ç« åˆ—è¡¨
    """
    articles = []
    
    print(f"ğŸ” ä½¿ç”¨ HTML è§£ææ–¹å¼æŠ“å–CSDNåšå®¢: {CSDN_USERNAME}")
    
    session = requests.Session()
    session.headers.update(HEADERS)
    
    max_retries = 3
    page = 1
    
    while True:
        for retry in range(max_retries):
            try:
                print(f"ğŸ“¡ æ­£åœ¨è¯·æ±‚ç¬¬ {page} é¡µ (å°è¯• {retry + 1}/{max_retries})...")
                
                # æ„å»ºåšå®¢åˆ—è¡¨URL
                url = f"{CSDN_BLOG_URL}/article/list/{page}"
                
                # å‘é€è¯·æ±‚
                response = session.get(url, timeout=15)
                response.raise_for_status()
                
                # ä½¿ç”¨ BeautifulSoup è§£æ HTML
                soup = BeautifulSoup(response.text, 'lxml')
                
                # æŸ¥æ‰¾æ–‡ç« åˆ—è¡¨
                article_items = soup.select('.article-item-box')
                
                if not article_items:
                    print(f"âœ… ç¬¬ {page} é¡µæ— æ›´å¤šæ–‡ç« ")
                    break
                
                print(f"âœ… ç¬¬ {page} é¡µæ‰¾åˆ° {len(article_items)} ç¯‡æ–‡ç« ")
                
                # è§£ææ¯ç¯‡æ–‡ç« 
                for item in article_items:
                    try:
                        # è·å–æ ‡é¢˜å’Œé“¾æ¥
                        title_elem = item.select_one('h4 a')
                        if not title_elem:
                            continue
                        
                        title = title_elem.get_text(strip=True)
                        link = title_elem.get('href', '')
                        
                        # ç¡®ä¿é“¾æ¥æ˜¯å®Œæ•´çš„
                        if link and not link.startswith('http'):
                            link = 'https://blog.csdn.net' + link
                        
                        # è·å–æ—¥æœŸ
                        date_elem = item.select_one('.date')
                        date_str = ''
                        if date_elem:
                            date_text = date_elem.get_text(strip=True)
                            # å°è¯•æå–æ—¥æœŸï¼Œæ ¼å¼å¯èƒ½æ˜¯ "2024-01-15" æˆ–å…¶ä»–
                            date_match = re.search(r'\d{4}-\d{2}-\d{2}', date_text)
                            if date_match:
                                date_str = date_match.group()
                        
                        # è·å–æ‘˜è¦
                        desc_elem = item.select_one('.content')
                        description = ''
                        if desc_elem:
                            description = desc_elem.get_text(strip=True)
                            if len(description) > 150:
                                description = description[:150] + '...'
                        
                        # è·å–é˜…è¯»é‡
                        views_elem = item.select_one('.read-num')
                        views = ''
                        if views_elem:
                            views_text = views_elem.get_text(strip=True)
                            # æå–æ•°å­—
                            views_match = re.search(r'\d+', views_text)
                            if views_match:
                                views = views_match.group()
                        
                        if title and link:
                            article = {
                                'title': title,
                                'link': link,
                                'date': date_str,
                                'excerpt': description,
                                'views': views
                            }
                            articles.append(article)
                        
                    except Exception as e:
                        print(f"âš ï¸  è§£ææ–‡ç« æ—¶å‡ºé”™: {str(e)}")
                        continue
                
                # æˆåŠŸè·å–ï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                break
                
            except Exception as e:
                print(f"âŒ è¯·æ±‚å¤±è´¥ (å°è¯• {retry + 1}/{max_retries}): {str(e)}")
                if retry < max_retries - 1:
                    time.sleep((retry + 1) * 3)
                else:
                    print(f"âš ï¸  ç¬¬ {page} é¡µè·å–å¤±è´¥ï¼Œè¿”å›å·²è·å–çš„ {len(articles)} ç¯‡æ–‡ç« ")
                    return articles
        else:
            # é‡è¯•å…¨éƒ¨å¤±è´¥
            break
        
        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ–‡ç« 
        if not article_items or len(article_items) == 0:
            break
        
        # ç»§ç»­è·å–ä¸‹ä¸€é¡µ
        page += 1
        time.sleep(2)  # ç¤¼è²Œåœ°ç­‰å¾…2ç§’
    
    print(f"\nâœ¨ æ€»å…±æŠ“å–åˆ° {len(articles)} ç¯‡æ–‡ç« ")
    return articles


def fetch_article_list_from_api():
    """
    ä½¿ç”¨ CSDN API æŠ“å–åšå®¢æ–‡ç« åˆ—è¡¨
    
    Returns:
        articles: æ–‡ç« åˆ—è¡¨
    """
    articles = []
    
    print(f"ğŸ” å¼€å§‹ä½¿ç”¨ API æŠ“å–CSDNåšå®¢: {CSDN_USERNAME}")
    
    session = requests.Session()
    session.headers.update(API_HEADERS)
    
    max_retries = 3
    page = 1
    page_size = 40  # æ¯é¡µè·å–40ç¯‡
    
    while True:
        for retry in range(max_retries):
            try:
                print(f"ğŸ“¡ æ­£åœ¨è¯·æ±‚ç¬¬ {page} é¡µ (å°è¯• {retry + 1}/{max_retries})...")
                
                # æ„å»º API è¯·æ±‚å‚æ•°
                params = {
                    'page': page,
                    'size': page_size,
                    'businessType': 'blog',
                    'orderby': '',
                    'noMore': 'false',
                    'year': '',
                    'month': '',
                    'username': CSDN_USERNAME
                }
                
                # å‘é€è¯·æ±‚åˆ° API
                response = session.get(CSDN_ARTICLE_LIST_API, params=params, timeout=15)
                response.raise_for_status()
                
                # è§£æ JSON å“åº”
                data = response.json()
                
                if data.get('code') != 200:
                    print(f"âš ï¸  API è¿”å›é”™è¯¯: {data.get('message', 'Unknown error')}")
                    if retry < max_retries - 1:
                        time.sleep((retry + 1) * 3)
                        continue
                    else:
                        break
                
                # è·å–æ–‡ç« åˆ—è¡¨
                article_list = data.get('data', {}).get('list', [])
                
                if not article_list:
                    print(f"âœ… ç¬¬ {page} é¡µæ— æ›´å¤šæ–‡ç« ï¼Œå·²è·å–å…¨éƒ¨")
                    break
                
                print(f"âœ… ç¬¬ {page} é¡µæ‰¾åˆ° {len(article_list)} ç¯‡æ–‡ç« ")
                
                # è§£ææ–‡ç« ä¿¡æ¯
                for item in article_list:
                    try:
                        title = item.get('title', '').strip()
                        article_id = item.get('articleId', '')
                        link = f"https://blog.csdn.net/{CSDN_USERNAME}/article/details/{article_id}"
                        
                        # å¤„ç†æ—¥æœŸ - è½¬æ¢æ—¶é—´æˆ³
                        post_time = item.get('postTime', '')
                        date_str = ''
                        if post_time:
                            try:
                                # CSDN è¿”å›çš„æ—¶é—´æˆ³æ˜¯æ¯«ç§’çº§
                                timestamp = int(post_time) / 1000 if len(str(post_time)) > 10 else int(post_time)
                                date_obj = datetime.fromtimestamp(timestamp)
                                date_str = date_obj.strftime('%Y-%m-%d')
                            except:
                                date_str = str(post_time)[:10]
                        
                        # è·å–æ‘˜è¦
                        description = item.get('description', '').strip()
                        if len(description) > 150:
                            description = description[:150] + '...'
                        
                        # è·å–é˜…è¯»é‡
                        view_count = item.get('viewCount', 0)
                        views = str(view_count) if view_count else ''
                        
                        if title and link:
                            article = {
                                'title': title,
                                'link': link,
                                'date': date_str,
                                'excerpt': description,
                                'views': views
                            }
                            articles.append(article)
                        
                    except Exception as e:
                        print(f"âš ï¸  è§£ææ–‡ç« æ—¶å‡ºé”™: {str(e)}")
                        continue
                
                # æˆåŠŸè·å–ï¼Œè·³å‡ºé‡è¯•å¾ªç¯
                break
                
            except requests.RequestException as e:
                print(f"âŒ è¯·æ±‚å¤±è´¥ (å°è¯• {retry + 1}/{max_retries}): {str(e)}")
                if retry < max_retries - 1:
                    time.sleep((retry + 1) * 5)
                else:
                    # æœ€åä¸€æ¬¡é‡è¯•ä¹Ÿå¤±è´¥äº†ï¼Œè¿”å›å·²è·å–çš„æ–‡ç« 
                    print(f"âš ï¸  ç¬¬ {page} é¡µè·å–å¤±è´¥ï¼Œè¿”å›å·²è·å–çš„ {len(articles)} ç¯‡æ–‡ç« ")
                    return articles
            except json.JSONDecodeError as e:
                print(f"âŒ JSON è§£æé”™è¯¯ (å°è¯• {retry + 1}/{max_retries}): {str(e)}")
                if retry < max_retries - 1:
                    time.sleep((retry + 1) * 3)
                else:
                    return articles
            except Exception as e:
                print(f"âŒ å¤„ç†æ—¶å‡ºé”™ (å°è¯• {retry + 1}/{max_retries}): {str(e)}")
                if retry < max_retries - 1:
                    time.sleep((retry + 1) * 3)
                else:
                    return articles
        else:
            # é‡è¯•å…¨éƒ¨å¤±è´¥
            break
        
        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤šæ–‡ç« 
        if len(article_list) < page_size:
            print(f"âœ… å·²è·å–æ‰€æœ‰æ–‡ç« ")
            break
        
        # ç»§ç»­è·å–ä¸‹ä¸€é¡µ
        page += 1
        time.sleep(2)  # ç¤¼è²Œåœ°ç­‰å¾…2ç§’å†è¯·æ±‚ä¸‹ä¸€é¡µ
    
    print(f"\nâœ¨ æ€»å…±æŠ“å–åˆ° {len(articles)} ç¯‡æ–‡ç« ")
    return articles


def save_to_yaml(articles):
    """
    å°†æ–‡ç« åˆ—è¡¨ä¿å­˜ä¸ºYAMLæ–‡ä»¶
    
    Args:
        articles: æ–‡ç« åˆ—è¡¨
    """
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)
    
    # æ·»åŠ å…ƒæ•°æ®
    data = {
        'last_updated': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'total_count': len(articles),
        'articles': articles
    }
    
    # ä¿å­˜ä¸ºYAML
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        yaml.dump(data, f, allow_unicode=True, sort_keys=False, default_flow_style=False)
    
    print(f"ğŸ’¾ æ–‡ç« åˆ—è¡¨å·²ä¿å­˜åˆ°: {OUTPUT_FILE}")


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("CSDNåšå®¢æ–‡ç« åŒæ­¥å·¥å…·")
    print("=" * 60)
    
    try:
        # é¦–å…ˆå°è¯•ä½¿ç”¨ API æ–¹æ³•
        print("\næ–¹æ³•1: å°è¯•ä½¿ç”¨ API...")
        articles = fetch_article_list_from_api()
        
        # å¦‚æœ API å¤±è´¥ï¼Œä½¿ç”¨ HTML è§£ææ–¹æ³•
        if not articles:
            print("\nâš ï¸  API æ–¹æ³•å¤±è´¥ï¼Œåˆ‡æ¢åˆ° HTML è§£ææ–¹æ³•...")
            print("æ–¹æ³•2: ä½¿ç”¨ HTML è§£æ...")
            articles = fetch_article_list_from_html()
        
        if articles:
            # ä¿å­˜åˆ°YAML
            save_to_yaml(articles)
            print("\nğŸ‰ åŒæ­¥å®Œæˆï¼")
            return 0
        else:
            print("\nâš ï¸  æœªæŠ“å–åˆ°ä»»ä½•æ–‡ç« ")
            # å³ä½¿æ²¡æœ‰æ–‡ç« ä¹Ÿä¸ç®—é”™è¯¯ï¼Œå¯èƒ½åšå®¢ç¡®å®æ˜¯ç©ºçš„
            # åˆ›å»ºä¸€ä¸ªç©ºçš„æ•°æ®æ–‡ä»¶
            save_to_yaml([])
            print("ğŸ’¾ å·²ä¿å­˜ç©ºçš„æ–‡ç« åˆ—è¡¨")
            return 0
            
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == '__main__':
    exit(main())
